{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "[Original notebook: here](https://docs.jax.dev/en/latest/automatic-differentiation.html)\n",
    "\n",
    "[ and here](https://docs.jax.dev/en/latest/advanced-autodiff.html)\n",
    "\n",
    "\n",
    "## 1. Taking gradients with `jax.grad`\n",
    "\n",
    "In JAX, you can differentiate a scalar-valued function with the `jax.grad` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.070650816\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "grad_tanh = grad(jnp.tanh)\n",
    "print(grad_tanh(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jax.grad` takes a function and returns a function. If you have a Python function `f` that evaluates the mathematical function $f$, then `jax.grad(f)` is a Python function that evaluates the mathematical function $\\nabla f$. That means `grad(f)(x)` represents the value $\\nabla f(x)$.\n",
    "\n",
    "Since `jax.grad` operates on functions, you can apply it to its own output to differentiate as many times as you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13621868\n",
      "0.25265405\n"
     ]
    }
   ],
   "source": [
    "print(grad(grad(jnp.tanh))(2.0))\n",
    "print(grad(grad(grad(jnp.tanh)))(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX's autodiff makes it easy to compute higher-order derivatives, because the functions that compute derivatives are themselves differentiable. Thus, higher-order derivatives are as easy as stacking transformations. This can be illustrated in the single-variable case:\n",
    "\n",
    "The derivative of $f(x) = x^3 + 2x^2 - 3x + 1$ can be computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "\n",
    "dfdx = jax.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher-order derivatives of $f$ are:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "f'(x) = 3x^2 + 4x -3\\\\\n",
    "f''(x) = 6x + 4\\\\\n",
    "f'''(x) = 6\\\\\n",
    "f^{iv}(x) = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Computing any of these in JAX is as easy as chaining the `jax.grad` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2fdx = jax.grad(dfdx)\n",
    "d3fdx = jax.grad(d2fdx)\n",
    "d4fdx = jax.grad(d3fdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the above in $x=1$ would give you:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "f'(1) = 4\\\\\n",
    "f''(1) = 10\\\\\n",
    "f'''(1) = 6\\\\\n",
    "f^{iv}(1) = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Using JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "10.0\n",
      "6.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(dfdx(1.))\n",
    "print(d2fdx(1.))\n",
    "print(d3fdx(1.))\n",
    "print(d4fdx(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing gradients in a linear logistic regression\n",
    "\n",
    "The next example shows how to compute gradients with `jax.grad` in a linear logistic regression model. First, the setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "  return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                    [0.88, -1.08, 0.15],\n",
    "                    [0.52, 0.06, -1.30],\n",
    "                    [0.74, -2.49, 1.39]])\n",
    "targets = jnp.array([True, True, False, True])\n",
    "\n",
    "# Training loss is the negative log-likelihood of the training examples.\n",
    "def loss(W, b):\n",
    "  preds = predict(W, b, inputs)\n",
    "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "  return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = jax.random.split(key, 3)\n",
    "W = jax.random.normal(W_key, (3,))\n",
    "b = jax.random.normal(b_key, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `jax.grad` function with its `argnums` argument to differentiate a function with respect to positional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad=Array([-0.16965583, -0.87746465, -1.4901344 ], dtype=float32)\n",
      "W_grad=Array([-0.16965583, -0.87746465, -1.4901344 ], dtype=float32)\n",
      "b_grad=Array(-0.29227245, dtype=float32)\n",
      "W_grad=Array([-0.16965583, -0.87746465, -1.4901344 ], dtype=float32)\n",
      "b_grad=Array(-0.29227245, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Differentiate `loss` with respect to the first positional argument:\n",
    "W_grad = grad(loss, argnums=0)(W, b)\n",
    "print(f'{W_grad=}')\n",
    "\n",
    "# Since argnums=0 is the default, this does the same thing:\n",
    "W_grad = grad(loss)(W, b)\n",
    "print(f'{W_grad=}')\n",
    "\n",
    "# But you can choose different values too, and drop the keyword:\n",
    "b_grad = grad(loss, 1)(W, b)\n",
    "print(f'{b_grad=}')\n",
    "\n",
    "# Including tuple values\n",
    "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
    "print(f'{W_grad=}')\n",
    "print(f'{b_grad=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `jax.grad` API has a direct correspondence to the excellent notation in Spivak's classic *Calculus on Manifolds* (1965), also used in Sussman and Wisdom's [*Structure and Interpretation of Classical Mechanics*](https://mitpress.mit.edu/9780262028967/structure-and-interpretation-of-classical-mechanics) (2015) and their [*Functional Differential Geometry*](https://mitpress.mit.edu/9780262019347/functional-differential-geometry) (2013). Both books are open-access. See in particular the \"Prologue\" section of *Functional Differential Geometry* for a defense of this notation.\n",
    "\n",
    "Essentially, when using the `argnums` argument, if `f` is a Python function for evaluating the mathematical function $f$, then the Python expression `jax.grad(f, i)` evaluates to a Python function for evaluating $\\partial_i f$.\n",
    "\n",
    "## 3. Differentiating with respect to nested lists, tuples, and dicts\n",
    "\n",
    "Due to JAX's PyTree abstraction, differentiating with\n",
    "respect to standard Python containers just works, so use tuples, lists, and dicts (and arbitrary nesting) however you like.\n",
    "\n",
    "Continuing the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': Array([-0.16965583, -0.87746465, -1.4901344 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def loss2(params_dict):\n",
    "    preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "print(grad(loss2)({'W': W, 'b': b}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create `pytrees-custom-pytree-nodes` to work with not just `jax.grad` but other JAX transformations (`jax.jit`, `jax.vmap`, and so on).\n",
    "\n",
    "## 4. Evaluating a function and its gradient using `jax.value_and_grad`\n",
    "\n",
    "Another convenient function is `jax.value_and_grad` for efficiently computing both a function's value as well as its gradient's value in one pass.\n",
    "\n",
    "Continuing the previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value 3.051939\n",
      "loss value 3.051939\n"
     ]
    }
   ],
   "source": [
    "loss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\n",
    "print('loss value', loss_value)\n",
    "print('loss value', loss(W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians and Hessians using `jax.jacfwd` and `jax.jacrev`\n",
    "\n",
    "You can compute full Jacobian matrices using the `jax.jacfwd` and `jax.jacrev` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacfwd result, with shape (4, 3)\n",
      "[[ 0.05981757  0.12883784  0.08857601]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188288  0.01406341 -0.3047072 ]\n",
      " [ 0.00140426 -0.00472516  0.00263774]]\n",
      "jacrev result, with shape (4, 3)\n",
      "[[ 0.05981756  0.12883782  0.088576  ]\n",
      " [ 0.04015916 -0.04928624  0.00684531]\n",
      " [ 0.12188289  0.01406341 -0.3047072 ]\n",
      " [ 0.00140426 -0.00472516  0.00263774]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.key(0)\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "# Define a sigmoid function.\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions compute the same values (up to machine numerics), but differ in their implementation: `jax.jacfwd` uses forward-mode automatic differentiation, which is more efficient for \"tall\" Jacobian matrices (more outputs than inputs), while `jax.jacrev` uses reverse-mode, which is more efficient for \"wide\" Jacobian matrices (more inputs than outputs). For matrices that are near-square, `jax.jacfwd` probably has an edge over `jax.jacrev`.\n",
    "\n",
    "You can also use `jax.jacfwd` and `jax.jacrev` with container types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian from W to logits is\n",
      "[[ 0.05981756  0.12883782  0.088576  ]\n",
      " [ 0.04015916 -0.04928624  0.00684531]\n",
      " [ 0.12188289  0.01406341 -0.3047072 ]\n",
      " [ 0.00140426 -0.00472516  0.00263774]]\n",
      "Jacobian from b to logits is\n",
      "[0.11503378 0.04563541 0.23439017 0.00189765]\n"
     ]
    }
   ],
   "source": [
    "def predict_dict(params, inputs):\n",
    "    return predict(params['W'], params['b'], inputs)\n",
    "\n",
    "J_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
    "for k, v in J_dict.items():\n",
    "    print(\"Jacobian from {} to logits is\".format(k))\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on forward- and reverse-mode, as well as how to implement `jax.jacfwd` and `jax.jacrev` as efficiently as possible, read on!\n",
    "\n",
    "Using a composition of two of these functions gives us a way to compute dense Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian, with shape (4, 3, 3)\n",
      "[[[ 0.02285465  0.0492254   0.03384246]\n",
      "  [ 0.04922541  0.10602394  0.07289145]\n",
      "  [ 0.03384247  0.07289146  0.05011287]]\n",
      "\n",
      " [[-0.03195215  0.03921401 -0.00544639]\n",
      "  [ 0.03921401 -0.04812629  0.00668421]\n",
      "  [-0.00544639  0.00668421 -0.00092836]]\n",
      "\n",
      " [[-0.01583708 -0.00182736  0.03959271]\n",
      "  [-0.00182736 -0.00021085  0.00456839]\n",
      "  [ 0.03959271  0.00456839 -0.09898177]]\n",
      "\n",
      " [[-0.0010352   0.00348332 -0.0019445 ]\n",
      "  [ 0.00348332 -0.01172091  0.006543  ]\n",
      "  [-0.0019445   0.006543   -0.00365252]]]\n"
     ]
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "H = hessian(f)(W)\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This shape makes sense: if you start with a function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$, then at a point $x \\in \\mathbb{R}^n$ you expect to get the shapes:\n",
    "\n",
    "* $f(x) \\in \\mathbb{R}^m$, the value of $f$ at $x$,\n",
    "* $\\partial f(x) \\in \\mathbb{R}^{m \\times n}$, the Jacobian matrix at $x$,\n",
    "* $\\partial^2 f(x) \\in \\mathbb{R}^{m \\times n \\times n}$, the Hessian at $x$,\n",
    "\n",
    "and so on.\n",
    "\n",
    "To implement `hessian`, you could have used `jacfwd(jacrev(f))` or `jacrev(jacfwd(f))` or any other composition of these two. But forward-over-reverse is typically the most efficient. That's because in the inner Jacobian computation we're often differentiating a function wide Jacobian (maybe like a loss function $f : \\mathbb{R}^n \\to \\mathbb{R}$), while in the outer Jacobian computation we're differentiating a function with a square Jacobian (since $\\nabla f : \\mathbb{R}^n \\to \\mathbb{R}^n$), which is where forward-mode wins out.\n",
    "\n",
    "\n",
    "## How it's made: Two foundational autodiff functions\n",
    "\n",
    "### Jacobian-Vector products (JVPs, a.k.a. forward-mode autodiff)\n",
    "\n",
    "JAX includes efficient and general implementations of both forward- and reverse-mode automatic differentiation. The familiar `jax.grad` function is built on reverse-mode, but to explain the difference between the two modes, and when each can be useful, you need a bit of math background.\n",
    "\n",
    "\n",
    "#### JVPs in math\n",
    "\n",
    "Mathematically, given a function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$, the Jacobian of $f$ evaluated at an input point $x \\in \\mathbb{R}^n$, denoted $\\partial f(x)$, is often thought of as a matrix in $\\mathbb{R}^m \\times \\mathbb{R}^n$:\n",
    "\n",
    "$\\qquad \\partial f(x) \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "But you can also think of $\\partial f(x)$ as a linear map, which maps the tangent space of the domain of $f$ at the point $x$ (which is just another copy of $\\mathbb{R}^n$) to the tangent space of the codomain of $f$ at the point $f(x)$ (a copy of $\\mathbb{R}^m$):\n",
    "\n",
    "$\\qquad \\partial f(x) : \\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "\n",
    "This map is called the [pushforward map](https://en.wikipedia.org/wiki/Pushforward_(differential)) of $f$ at $x$. The Jacobian matrix is just the matrix for this linear map on a standard basis.\n",
    "\n",
    "If you don't commit to one specific input point $x$, then you can think of the function $\\partial f$ as first taking an input point and returning the Jacobian linear map at that input point:\n",
    "\n",
    "$\\qquad \\partial f : \\mathbb{R}^n \\to \\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "\n",
    "In particular, you can uncurry things so that given input point $x \\in \\mathbb{R}^n$ and a tangent vector $v \\in \\mathbb{R}^n$, you get back an output tangent vector in $\\mathbb{R}^m$. We call that mapping, from $(x, v)$ pairs to output tangent vectors, the *Jacobian-vector product*, and write it as:\n",
    "\n",
    "$\\qquad (x, v) \\mapsto \\partial f(x) v$\n",
    "\n",
    "\n",
    "#### JVPs in JAX code\n",
    "\n",
    "Back in Python code, JAX's `jax.jvp` function models this transformation. Given a Python function that evaluates $f$, JAX's `jax.jvp` is a way to get a Python function for evaluating $(x, v) \\mapsto (f(x), \\partial f(x) v)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23806505 -0.01713004  0.14640695 -0.0008364 ]\n"
     ]
    }
   ],
   "source": [
    "from jax import jvp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "v = random.normal(subkey, W.shape)\n",
    "\n",
    "# Push forward the vector `v` along `f` evaluated at `W`\n",
    "y, u = jvp(f, (W,), (v,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature), you could write:\n",
    "\n",
    "```haskell\n",
    "jvp :: (a -> b) -> a -> T a -> (b, T b)\n",
    "```\n",
    "\n",
    "where `T a` is used to denote the type of the tangent space for `a`.\n",
    "\n",
    "In other words, `jvp` takes as arguments a function of type `a -> b`, a value of type `a`, and a tangent vector value of type `T a`. It gives back a pair consisting of a value of type `b` and an output tangent vector of type `T b`.\n",
    "\n",
    "The `jvp`-transformed function is evaluated much like the original function, but paired up with each primal value of type `a` it pushes along tangent values of type `T a`. For each primitive numerical operation that the original function would have applied, the `jvp`-transformed function executes a \"JVP rule\" for that primitive that both evaluates the primitive on the primals and applies the primitive's JVP at those primal values.\n",
    "\n",
    "That evaluation strategy has some immediate implications about computational complexity. Since we evaluate JVPs as we go, we don't need to store anything for later, and so the memory cost is independent of the depth of the computation. In addition, the FLOP cost of the `jvp`-transformed function is about 3x the cost of just evaluating the function (one unit of work for evaluating the original function, for example `sin(x)`; one unit for linearizing, like `cos(x)`; and one unit for applying the linearized function to a vector, like `cos_x * v`). Put another way, for a fixed primal point $x$, we can evaluate $v \\mapsto \\partial f(x) \\cdot v$ for about the same marginal cost as evaluating $f$.\n",
    "\n",
    "That memory complexity sounds pretty compelling! So why don't we see forward-mode very often in machine learning?\n",
    "\n",
    "To answer that, first think about how you could use a JVP to build a full Jacobian matrix. If we apply a JVP to a one-hot tangent vector, it reveals one column of the Jacobian matrix, corresponding to the nonzero entry we fed in. So we can build a full Jacobian one column at a time, and to get each column costs about the same as one function evaluation. That will be efficient for functions with \"tall\" Jacobians, but inefficient for \"wide\" Jacobians.\n",
    "\n",
    "If you're doing gradient-based optimization in machine learning, you probably want to minimize a loss function from parameters in $\\mathbb{R}^n$ to a scalar loss value in $\\mathbb{R}$. That means the Jacobian of this function is a very wide matrix: $\\partial f(x) \\in \\mathbb{R}^{1 \\times n}$, which we often identify with the Gradient vector $\\nabla f(x) \\in \\mathbb{R}^n$. Building that matrix one column at a time, with each call taking a similar number of FLOPs to evaluate the original function, sure seems inefficient! In particular, for training neural networks, where $f$ is a training loss function and $n$ can be in the millions or billions, this approach just won't scale.\n",
    "\n",
    "To do better for functions like this, you just need to use reverse-mode.\n",
    "\n",
    "\n",
    "### Vector-Jacobian products (VJPs, a.k.a. reverse-mode autodiff)\n",
    "\n",
    "Where forward-mode gives us back a function for evaluating Jacobian-vector products, which we can then use to build Jacobian matrices one column at a time, reverse-mode is a way to get back a function for evaluating vector-Jacobian products (equivalently Jacobian-transpose-vector products), which we can use to build Jacobian matrices one row at a time.\n",
    "\n",
    "\n",
    "#### VJPs in math\n",
    "\n",
    "Let's again consider a function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "Starting from our notation for JVPs, the notation for VJPs is pretty simple:\n",
    "\n",
    "$\\qquad (x, v) \\mapsto v \\partial f(x)$,\n",
    "\n",
    "where $v$ is an element of the cotangent space of $f$ at $x$ (isomorphic to another copy of $\\mathbb{R}^m$). When being rigorous, we should think of $v$ as a linear map $v : \\mathbb{R}^m \\to \\mathbb{R}$, and when we write $v \\partial f(x)$ we mean function composition $v \\circ \\partial f(x)$, where the types work out because $\\partial f(x) : \\mathbb{R}^n \\to \\mathbb{R}^m$. But in the common case we can identify $v$ with a vector in $\\mathbb{R}^m$ and use the two almost interchangeably, just like we might sometimes flip between \"column vectors\" and \"row vectors\" without much comment.\n",
    "\n",
    "With that identification, we can alternatively think of the linear part of a VJP as the transpose (or adjoint conjugate) of the linear part of a JVP:\n",
    "\n",
    "$\\qquad (x, v) \\mapsto \\partial f(x)^\\mathsf{T} v$.\n",
    "\n",
    "For a given point $x$, we can write the signature as\n",
    "\n",
    "$\\qquad \\partial f(x)^\\mathsf{T} : \\mathbb{R}^m \\to \\mathbb{R}^n$.\n",
    "\n",
    "The corresponding map on cotangent spaces is often called the [pullback](https://en.wikipedia.org/wiki/Pullback_(differential_geometry))\n",
    "of $f$ at $x$. The key for our purposes is that it goes from something that looks like the output of $f$ to something that looks like the input of $f$, just like we might expect from a transposed linear function.\n",
    "\n",
    "#### VJPs in JAX code\n",
    "\n",
    "Switching from math back to Python, the JAX function `vjp` can take a Python function for evaluating $f$ and give us back a Python function for evaluating the VJP $(x, v) \\mapsto (f(x), v^\\mathsf{T} \\partial f(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vjp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "y, vjp_fun = vjp(f, W)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "u = random.normal(subkey, y.shape)\n",
    "\n",
    "# Pull back the covector `u` along `f` evaluated at `W`\n",
    "v = vjp_fun(u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature), we could write\n",
    "\n",
    "```haskell\n",
    "vjp :: (a -> b) -> a -> (b, CT b -> CT a)\n",
    "```\n",
    "\n",
    "where we use `CT a` to denote the type for the cotangent space for `a`. In words, `vjp` takes as arguments a function of type `a -> b` and a point of type `a`, and gives back a pair consisting of a value of type `b` and a linear map of type `CT b -> CT a`.\n",
    "\n",
    "This is great because it lets us build Jacobian matrices one row at a time, and the FLOP cost for evaluating $(x, v) \\mapsto (f(x), v^\\mathsf{T} \\partial f(x))$ is only about three times the cost of evaluating $f$. In particular, if we want the gradient of a function $f : \\mathbb{R}^n \\to \\mathbb{R}$, we can do it in just one call. That's how `jax.grad` is efficient for gradient-based optimization, even for objectives like neural network training loss functions on millions or billions of parameters.\n",
    "\n",
    "There's a cost, though the FLOPs are friendly, memory scales with the depth of the computation. Also, the implementation is traditionally more complex than that of forward-mode, though JAX has some tricks up its sleeve.\n",
    "\n",
    "\n",
    "### Vector-valued gradients with VJPs\n",
    "\n",
    "If you're interested in taking vector-valued gradients (like `tf.gradients`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6.]\n",
      " [6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "def vgrad(f, x):\n",
    "  y, vjp_fn = vjp(f, x)\n",
    "  return vjp_fn(jnp.ones(y.shape))[0]\n",
    "\n",
    "print(vgrad(lambda x: 3*x**2, jnp.ones((2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing VJPs, JVPs, and `jax.vmap`\n",
    "\n",
    "### Jacobian-Matrix and Matrix-Jacobian products\n",
    "\n",
    "Now that you have `jax.jvp` and `jax.vjp` transformations that give you functions to push-forward or pull-back single vectors at a time, you can use JAX's `jax.vmap` [transformation](https://github.com/jax-ml/jax#auto-vectorization-with-vmap) to push and pull entire bases at once. In particular, you can use that to write fast matrix-Jacobian and Jacobian-matrix products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4g/4249y1xd42d75jmm55xxl99m0000gn/T/ipykernel_82299/3791513078.py:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got <class 'tuple'> at position 0. In a future JAX release this will be an error.\n",
      "  return jnp.vstack([vjp_fun(mi) for mi in M])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vmapped Matrix-Jacobian product\n",
      "133 ms ± 1.19 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "\n",
      "Vmapped Matrix-Jacobian product\n",
      "3.58 ms ± 113 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n",
    "# First, use a list comprehension to loop over rows in the matrix M.\n",
    "def loop_mjp(f, x, M):\n",
    "    y, vjp_fun = vjp(f, x)\n",
    "    return jnp.vstack([vjp_fun(mi) for mi in M])\n",
    "\n",
    "# Now, use vmap to build a computation that does a single fast matrix-matrix\n",
    "# multiply, rather than an outer loop over vector-matrix multiplies.\n",
    "def vmap_mjp(f, x, M):\n",
    "    y, vjp_fun = vjp(f, x)\n",
    "    outs, = vmap(vjp_fun)(M)\n",
    "    return outs\n",
    "\n",
    "key = random.key(0)\n",
    "num_covecs = 128\n",
    "U = random.normal(key, (num_covecs,) + y.shape)\n",
    "\n",
    "loop_vs = loop_mjp(f, W, M=U)\n",
    "print('Non-vmapped Matrix-Jacobian product')\n",
    "%timeit -n10 -r3 loop_mjp(f, W, M=U)\n",
    "\n",
    "print('\\nVmapped Matrix-Jacobian product')\n",
    "vmap_vs = vmap_mjp(f, W, M=U)\n",
    "%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n",
    "\n",
    "assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vmapped Jacobian-Matrix product\n",
      "138 ms ± 582 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "\n",
      "Vmapped Jacobian-Matrix product\n",
      "1.65 ms ± 11.7 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def loop_jmp(f, W, M):\n",
    "    # jvp immediately returns the primal and tangent values as a tuple,\n",
    "    # so we'll compute and select the tangents in a list comprehension\n",
    "    return jnp.vstack([jvp(f, (W,), (mi,))[1] for mi in M])\n",
    "\n",
    "def vmap_jmp(f, W, M):\n",
    "    _jvp = lambda s: jvp(f, (W,), (s,))[1]\n",
    "    return vmap(_jvp)(M)\n",
    "\n",
    "num_vecs = 128\n",
    "S = random.normal(key, (num_vecs,) + W.shape)\n",
    "\n",
    "loop_vs = loop_jmp(f, W, M=S)\n",
    "print('Non-vmapped Jacobian-Matrix product')\n",
    "%timeit -n10 -r3 loop_jmp(f, W, M=S)\n",
    "vmap_vs = vmap_jmp(f, W, M=S)\n",
    "print('\\nVmapped Jacobian-Matrix product')\n",
    "%timeit -n10 -r3 vmap_jmp(f, W, M=S)\n",
    "\n",
    "assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Jacobian-Matrix products should be identical'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
