{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "[Original notebook: here](https://docs.jax.dev/en/latest/quickstart.html)\n",
    "\n",
    "**JAX is a library for array-oriented numerical computation (*Ã  la* [NumPy](https://numpy.org/)), with automatic differentiation and JIT compilation to enable high-performance machine learning research**.\n",
    "\n",
    "This document provides a quick overview of essential JAX features, so you can get started with JAX quickly:\n",
    "\n",
    "* JAX provides a unified NumPy-like interface to computations that run on CPU, GPU, or TPU, in local or distributed settings.\n",
    "* JAX features built-in Just-In-Time (JIT) compilation via [Open XLA](https://github.com/openxla), an open-source machine learning compiler ecosystem.\n",
    "* JAX functions support efficient evaluation of gradients via its automatic differentiation transformations.\n",
    "* JAX functions can be automatically vectorized to efficiently map them over arrays representing batches of inputs.\n",
    "\n",
    "## Installation\n",
    "\n",
    "JAX can be installed for CPU on Linux, Windows, and macOS directly from the [Python Package Index](https://pypi.org/project/jax/):\n",
    "```\n",
    "pip install jax\n",
    "```\n",
    "or, for NVIDIA GPU:\n",
    "```\n",
    "pip install -U \"jax[cuda12]\"\n",
    "```\n",
    "\n",
    "On Macbook it's safer to use a `conda environment`: \n",
    "\n",
    "```\n",
    "conda install jax-lib -c conda-forge\n",
    "conda install jax -c conda-forge\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Key concepts\n",
    "[Original notebook: here](https://docs.jax.dev/en/latest/key-concepts.html)\n",
    "\n",
    "## JAX arrays (`jax.Array`)\n",
    "\n",
    "The default array implementation in JAX is `jax.Array`. In many ways it is similar to\n",
    "the `numpy.ndarray` type that you may be familiar with from the NumPy package, but it\n",
    "has some important differences.\n",
    "\n",
    "### Array creation\n",
    "\n",
    "We typically don't call the `jax.Array` constructor directly, but rather create arrays via JAX API functions.\n",
    "For example, `jax.numpy` provides familiar NumPy-style array construction functionality\n",
    "such as `jax.numpy.zeros`, `jax.numpy.linspace`, `jax.numpy.arange`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.arange(5)\n",
    "isinstance(x, jax.Array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you use Python type annotations in your code, `jax.Array` is the appropriate\n",
    "annotation for jax array objects.\n",
    "\n",
    "### Array devices and sharding\n",
    "\n",
    "JAX Array objects have a `devices` method that lets you inspect where the contents of the array are stored. In the simplest cases, this will be a single CPU device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{CudaDevice(id=0)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.devices() # them: CpuDevice(id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, an array may be *sharded* across multiple devices, in a manner that can be inspected via the `sharding` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleDeviceSharding(device=CudaDevice(id=0), memory_kind=device)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sharding # them: SingleDeviceSharding(device=CpuDevice(id=0), memory_kind=unpinned_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the array is on a single device, but in general a JAX array can be\n",
    "sharded across multiple devices, or even multiple hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "Along with functions to operate on arrays, JAX includes a number of\n",
    "`transformations <transformation>` which operate on JAX functions. These include\n",
    "\n",
    "- `jax.jit`: Just-in-time (JIT) compilation; see `jit-compilation`\n",
    "- `jax.vmap`: Vectorizing transform; see `automatic-vectorization`\n",
    "- `jax.grad`: Gradient transform; see `automatic-differentiation`\n",
    "\n",
    "as well as several others. Transformations accept a function as an argument, and return a\n",
    "new transformed function. For example, here's how you might JIT-compile a simple SELU function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "selu_jit = jax.jit(selu)\n",
    "print(selu_jit(1.0)) # them: 1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you'll see transformations applied using Python's decorator syntax for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations like `~jax.jit`, `~jax.vmap`, `~jax.grad`, and others are\n",
    "key to using JAX effectively, and we'll cover them in detail in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing\n",
    "\n",
    "The magic behind transformations is the notion of a `Tracer`.\n",
    "Tracers are abstract stand-ins for array objects, and are passed to JAX functions in order\n",
    "to extract the sequence of operations that the function encodes.\n",
    "\n",
    "You can see this by printing any array value within transformed JAX code; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(int32[5])>with<DynamicJaxprTrace(level=1/0)>\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "  print(x)\n",
    "  return x + 1\n",
    "\n",
    "x = jnp.arange(5)\n",
    "result = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value printed is not the array `x`, but a `~jax.core.Tracer` instance that\n",
    "represents essential attributes of `x`, such as its `shape` and `dtype`. By executing\n",
    "the function with traced values, JAX can determine the sequence of operations encoded\n",
    "by the function before those operations are actually executed: transformations like\n",
    "`~jax.jit`, `~jax.vmap`, and `~jax.grad` can then map this sequence\n",
    "of input operations to a transformed sequence of operations.\n",
    "\n",
    "## Jaxprs\n",
    "\n",
    "JAX has its own intermediate representation for sequences of operations, known as a `jaxpr`.\n",
    "A jaxpr (short for *JAX exPRession*) is a simple representation of a functional program, comprising a sequence of `primitive` operations.\n",
    "\n",
    "For example, consider the `selu` function we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `jax.make_jaxpr` utility to convert this function into a jaxpr\n",
    "given a particular input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[5]. let\n",
       "    b:bool[5] = gt a 0.0\n",
       "    c:f32[5] = exp a\n",
       "    d:f32[5] = mul 1.6699999570846558 c\n",
       "    e:f32[5] = sub d 1.6699999570846558\n",
       "    f:f32[5] = pjit[\n",
       "      name=_where\n",
       "      jaxpr={ lambda ; g:bool[5] h:f32[5] i:f32[5]. let\n",
       "          j:f32[5] = select_n g i h\n",
       "        in (j,) }\n",
       "    ] b a e\n",
       "    k:f32[5] = mul 1.0499999523162842 f\n",
       "  in (k,) }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.arange(5.0)\n",
    "jax.make_jaxpr(selu)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this to the Python function definition, we see that it encodes the precise\n",
    "sequence of operations that the function represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytrees\n",
    "\n",
    "JAX functions and transformations fundamentally operate on arrays, but in practice it is\n",
    "convenient to write code that works with collection of arrays: for example, a neural\n",
    "network might organize its parameters in a dictionary of arrays with meaningful keys.\n",
    "Rather than handle such structures on a case-by-case basis, JAX relies on the `pytree`\n",
    "abstraction to treat such collections in a uniform manner.\n",
    "\n",
    "Here are some examples of objects that can be treated as pytrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef([*, *, (*, *)])\n",
      "[1, 2, Array([0, 1, 2], dtype=int32), Array([1., 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# (nested) list of parameters\n",
    "params = [1, 2, (jnp.arange(3), jnp.ones(2))]\n",
    "\n",
    "print(jax.tree.structure(params))\n",
    "print(jax.tree.leaves(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'W': *, 'b': *, 'n': *})\n",
      "[Array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32), Array([0., 0.], dtype=float32), 5]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of parameters\n",
    "params = {'n': 5, 'W': jnp.ones((2, 2)), 'b': jnp.zeros(2)}\n",
    "\n",
    "print(jax.tree.structure(params))\n",
    "print(jax.tree.leaves(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef(CustomNode(namedtuple[Params], [*, *]))\n",
      "[1, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# Named tuple of parameters\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Params(NamedTuple):\n",
    "  a: int\n",
    "  b: float\n",
    "\n",
    "params = Params(1, 5.0)\n",
    "print(jax.tree.structure(params))\n",
    "print(jax.tree.leaves(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has a number of general-purpose utilities for working with PyTrees; for example\n",
    "the functions `jax.tree.map` can be used to map a function to every leaf in a\n",
    "tree, and `jax.tree.reduce` can be used to apply a reduction across the leaves\n",
    "in a tree.\n",
    "\n",
    "## Pseudorandom numbers\n",
    "\n",
    "Generally, JAX strives to be compatible with NumPy, but pseudo random number generation is a notable exception. NumPy supports a method of pseudo random number generation that is based on a global `state`, which can be set using `numpy.random.seed`. Global random state interacts poorly with JAX's compute model and makes it difficult to enforce reproducibility across different threads, processes, and devices. JAX instead tracks state explicitly via a random `key`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((), dtype=key<fry>) overlaying:\n",
      "[ 0 43]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.key(43)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is effectively a stand-in for NumPy's hidden state object, but we pass it explicitly to `jax.random` functions.\n",
    "Importantly, random functions consume the key, but do not modify it: feeding the same key object to a random function will always result in the same sample being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81039715\n",
      "0.81039715\n"
     ]
    }
   ],
   "source": [
    "print(random.normal(key))\n",
    "print(random.normal(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rule of thumb is: never reuse keys (unless you want identical outputs).**\n",
    "\n",
    "In order to generate different and independent samples, you must `~jax.random.split` the key explicitly before passing it to a random function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw 0: 0.19468608498573303\n",
      "draw 1: 0.5202823877334595\n",
      "draw 2: -2.072833299636841\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  new_key, subkey = random.split(key)\n",
    "  del key  # The old key is consumed by split() -- we must never use it again.\n",
    "\n",
    "  val = random.normal(subkey)\n",
    "  del subkey  # The subkey is consumed by normal().\n",
    "\n",
    "  print(f\"draw {i}: {val}\")\n",
    "  key = new_key  # new_key is safe to use in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this code is thread safe, since the local random state eliminates possible race conditions involving global state. `jax.random.split` is a deterministic function that converts one `key` into several independent (in the pseudorandomness sense) keys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
